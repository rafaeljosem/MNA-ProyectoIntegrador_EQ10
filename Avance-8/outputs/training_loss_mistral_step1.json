[
    {
        "epoch": 0.007942811755361398,
        "grad_norm": 1.9090019464492798,
        "learning_rate": 1e-05,
        "loss": 1.2182,
        "step": 10
    },
    {
        "epoch": 0.015885623510722795,
        "grad_norm": 1.2005115747451782,
        "learning_rate": 2e-05,
        "loss": 1.0207,
        "step": 20
    },
    {
        "epoch": 0.023828435266084195,
        "grad_norm": 1.6505759954452515,
        "learning_rate": 1.983857949959645e-05,
        "loss": 0.8661,
        "step": 30
    },
    {
        "epoch": 0.03177124702144559,
        "grad_norm": 0.890368640422821,
        "learning_rate": 1.96771589991929e-05,
        "loss": 0.8434,
        "step": 40
    },
    {
        "epoch": 0.03971405877680699,
        "grad_norm": 1.0244274139404297,
        "learning_rate": 1.9515738498789348e-05,
        "loss": 0.8149,
        "step": 50
    },
    {
        "epoch": 0.04765687053216839,
        "grad_norm": 0.9755045175552368,
        "learning_rate": 1.9354317998385797e-05,
        "loss": 0.8302,
        "step": 60
    },
    {
        "epoch": 0.05559968228752978,
        "grad_norm": 1.0832377672195435,
        "learning_rate": 1.9192897497982246e-05,
        "loss": 0.7712,
        "step": 70
    },
    {
        "epoch": 0.06354249404289118,
        "grad_norm": 0.9869239926338196,
        "learning_rate": 1.9031476997578695e-05,
        "loss": 0.7781,
        "step": 80
    },
    {
        "epoch": 0.07148530579825259,
        "grad_norm": 1.2412011623382568,
        "learning_rate": 1.8870056497175144e-05,
        "loss": 0.7294,
        "step": 90
    },
    {
        "epoch": 0.07942811755361398,
        "grad_norm": 1.0499846935272217,
        "learning_rate": 1.8708635996771592e-05,
        "loss": 0.7513,
        "step": 100
    },
    {
        "epoch": 0.08737092930897537,
        "grad_norm": 1.0328952074050903,
        "learning_rate": 1.854721549636804e-05,
        "loss": 0.7652,
        "step": 110
    },
    {
        "epoch": 0.09531374106433678,
        "grad_norm": 1.1581004858016968,
        "learning_rate": 1.838579499596449e-05,
        "loss": 0.7555,
        "step": 120
    },
    {
        "epoch": 0.10325655281969817,
        "grad_norm": 1.0789419412612915,
        "learning_rate": 1.8224374495560936e-05,
        "loss": 0.6943,
        "step": 130
    },
    {
        "epoch": 0.11119936457505956,
        "grad_norm": 1.0639315843582153,
        "learning_rate": 1.8062953995157388e-05,
        "loss": 0.7672,
        "step": 140
    },
    {
        "epoch": 0.11914217633042097,
        "grad_norm": 1.2451674938201904,
        "learning_rate": 1.7901533494753837e-05,
        "loss": 0.7704,
        "step": 150
    },
    {
        "epoch": 0.12708498808578236,
        "grad_norm": 1.1666171550750732,
        "learning_rate": 1.7740112994350286e-05,
        "loss": 0.717,
        "step": 160
    },
    {
        "epoch": 0.13502779984114377,
        "grad_norm": 0.9387497901916504,
        "learning_rate": 1.757869249394673e-05,
        "loss": 0.7418,
        "step": 170
    },
    {
        "epoch": 0.14297061159650518,
        "grad_norm": 0.9801112413406372,
        "learning_rate": 1.741727199354318e-05,
        "loss": 0.7607,
        "step": 180
    },
    {
        "epoch": 0.15091342335186655,
        "grad_norm": 1.0333560705184937,
        "learning_rate": 1.7255851493139632e-05,
        "loss": 0.7347,
        "step": 190
    },
    {
        "epoch": 0.15885623510722796,
        "grad_norm": 0.9990265965461731,
        "learning_rate": 1.709443099273608e-05,
        "loss": 0.7814,
        "step": 200
    },
    {
        "epoch": 0.16679904686258937,
        "grad_norm": 0.984190821647644,
        "learning_rate": 1.6933010492332526e-05,
        "loss": 0.7222,
        "step": 210
    },
    {
        "epoch": 0.17474185861795075,
        "grad_norm": 1.10776686668396,
        "learning_rate": 1.6771589991928975e-05,
        "loss": 0.7598,
        "step": 220
    },
    {
        "epoch": 0.18268467037331215,
        "grad_norm": 1.0950114727020264,
        "learning_rate": 1.6610169491525424e-05,
        "loss": 0.6971,
        "step": 230
    },
    {
        "epoch": 0.19062748212867356,
        "grad_norm": 1.1336311101913452,
        "learning_rate": 1.6448748991121873e-05,
        "loss": 0.6915,
        "step": 240
    },
    {
        "epoch": 0.19857029388403494,
        "grad_norm": 0.9996558427810669,
        "learning_rate": 1.6287328490718322e-05,
        "loss": 0.659,
        "step": 250
    },
    {
        "epoch": 0.20651310563939634,
        "grad_norm": 1.1452701091766357,
        "learning_rate": 1.612590799031477e-05,
        "loss": 0.6993,
        "step": 260
    },
    {
        "epoch": 0.21445591739475775,
        "grad_norm": 0.9613982439041138,
        "learning_rate": 1.596448748991122e-05,
        "loss": 0.6986,
        "step": 270
    },
    {
        "epoch": 0.22239872915011913,
        "grad_norm": 1.219883918762207,
        "learning_rate": 1.5803066989507668e-05,
        "loss": 0.6721,
        "step": 280
    },
    {
        "epoch": 0.23034154090548054,
        "grad_norm": 1.1800340414047241,
        "learning_rate": 1.5641646489104117e-05,
        "loss": 0.6553,
        "step": 290
    },
    {
        "epoch": 0.23828435266084194,
        "grad_norm": 1.0532917976379395,
        "learning_rate": 1.5480225988700566e-05,
        "loss": 0.6342,
        "step": 300
    },
    {
        "epoch": 0.24622716441620335,
        "grad_norm": 1.5549277067184448,
        "learning_rate": 1.5318805488297015e-05,
        "loss": 0.6196,
        "step": 310
    },
    {
        "epoch": 0.2541699761715647,
        "grad_norm": 1.232008934020996,
        "learning_rate": 1.5157384987893464e-05,
        "loss": 0.7067,
        "step": 320
    },
    {
        "epoch": 0.2621127879269261,
        "grad_norm": 1.1098493337631226,
        "learning_rate": 1.4995964487489914e-05,
        "loss": 0.7123,
        "step": 330
    },
    {
        "epoch": 0.27005559968228754,
        "grad_norm": 1.0703730583190918,
        "learning_rate": 1.4834543987086361e-05,
        "loss": 0.6675,
        "step": 340
    },
    {
        "epoch": 0.2779984114376489,
        "grad_norm": 1.1391983032226562,
        "learning_rate": 1.467312348668281e-05,
        "loss": 0.6788,
        "step": 350
    },
    {
        "epoch": 0.28594122319301035,
        "grad_norm": 1.215882420539856,
        "learning_rate": 1.4511702986279259e-05,
        "loss": 0.6747,
        "step": 360
    },
    {
        "epoch": 0.29388403494837173,
        "grad_norm": 1.302123785018921,
        "learning_rate": 1.4350282485875708e-05,
        "loss": 0.6621,
        "step": 370
    },
    {
        "epoch": 0.3018268467037331,
        "grad_norm": 1.2965415716171265,
        "learning_rate": 1.4188861985472155e-05,
        "loss": 0.6377,
        "step": 380
    },
    {
        "epoch": 0.30976965845909454,
        "grad_norm": 1.090097427368164,
        "learning_rate": 1.4027441485068604e-05,
        "loss": 0.6459,
        "step": 390
    },
    {
        "epoch": 0.3177124702144559,
        "grad_norm": 1.2889435291290283,
        "learning_rate": 1.3866020984665054e-05,
        "loss": 0.6541,
        "step": 400
    },
    {
        "epoch": 0.3256552819698173,
        "grad_norm": 1.2349385023117065,
        "learning_rate": 1.3704600484261503e-05,
        "loss": 0.6575,
        "step": 410
    },
    {
        "epoch": 0.33359809372517873,
        "grad_norm": 0.9684121608734131,
        "learning_rate": 1.354317998385795e-05,
        "loss": 0.6614,
        "step": 420
    },
    {
        "epoch": 0.3415409054805401,
        "grad_norm": 1.3065494298934937,
        "learning_rate": 1.33817594834544e-05,
        "loss": 0.6658,
        "step": 430
    },
    {
        "epoch": 0.3494837172359015,
        "grad_norm": 1.2707871198654175,
        "learning_rate": 1.3220338983050848e-05,
        "loss": 0.6393,
        "step": 440
    },
    {
        "epoch": 0.3574265289912629,
        "grad_norm": 1.162940502166748,
        "learning_rate": 1.3058918482647299e-05,
        "loss": 0.6644,
        "step": 450
    },
    {
        "epoch": 0.3653693407466243,
        "grad_norm": 1.2749435901641846,
        "learning_rate": 1.2897497982243746e-05,
        "loss": 0.6631,
        "step": 460
    },
    {
        "epoch": 0.3733121525019857,
        "grad_norm": 1.4536458253860474,
        "learning_rate": 1.2736077481840195e-05,
        "loss": 0.6237,
        "step": 470
    },
    {
        "epoch": 0.3812549642573471,
        "grad_norm": 1.3198884725570679,
        "learning_rate": 1.2574656981436644e-05,
        "loss": 0.5952,
        "step": 480
    },
    {
        "epoch": 0.3891977760127085,
        "grad_norm": 1.491672396659851,
        "learning_rate": 1.2413236481033092e-05,
        "loss": 0.6097,
        "step": 490
    },
    {
        "epoch": 0.3971405877680699,
        "grad_norm": 1.3174939155578613,
        "learning_rate": 1.225181598062954e-05,
        "loss": 0.6631,
        "step": 500
    },
    {
        "epoch": 0.4050833995234313,
        "grad_norm": 1.3669817447662354,
        "learning_rate": 1.209039548022599e-05,
        "loss": 0.6643,
        "step": 510
    },
    {
        "epoch": 0.4130262112787927,
        "grad_norm": 1.43507719039917,
        "learning_rate": 1.1928974979822439e-05,
        "loss": 0.5952,
        "step": 520
    },
    {
        "epoch": 0.42096902303415407,
        "grad_norm": 1.198228359222412,
        "learning_rate": 1.1767554479418888e-05,
        "loss": 0.6546,
        "step": 530
    },
    {
        "epoch": 0.4289118347895155,
        "grad_norm": 1.097327709197998,
        "learning_rate": 1.1606133979015335e-05,
        "loss": 0.6182,
        "step": 540
    },
    {
        "epoch": 0.4368546465448769,
        "grad_norm": 1.5026103258132935,
        "learning_rate": 1.1444713478611784e-05,
        "loss": 0.6301,
        "step": 550
    },
    {
        "epoch": 0.44479745830023826,
        "grad_norm": 1.332399845123291,
        "learning_rate": 1.1283292978208233e-05,
        "loss": 0.6469,
        "step": 560
    },
    {
        "epoch": 0.4527402700555997,
        "grad_norm": 1.221882700920105,
        "learning_rate": 1.1121872477804683e-05,
        "loss": 0.6323,
        "step": 570
    },
    {
        "epoch": 0.46068308181096107,
        "grad_norm": 1.4545230865478516,
        "learning_rate": 1.096045197740113e-05,
        "loss": 0.6357,
        "step": 580
    },
    {
        "epoch": 0.4686258935663225,
        "grad_norm": 1.49323570728302,
        "learning_rate": 1.079903147699758e-05,
        "loss": 0.6096,
        "step": 590
    },
    {
        "epoch": 0.4765687053216839,
        "grad_norm": 1.4124348163604736,
        "learning_rate": 1.0637610976594028e-05,
        "loss": 0.6237,
        "step": 600
    },
    {
        "epoch": 0.48451151707704526,
        "grad_norm": 1.3455945253372192,
        "learning_rate": 1.0476190476190477e-05,
        "loss": 0.6232,
        "step": 610
    },
    {
        "epoch": 0.4924543288324067,
        "grad_norm": 1.6008769273757935,
        "learning_rate": 1.0314769975786927e-05,
        "loss": 0.539,
        "step": 620
    },
    {
        "epoch": 0.5003971405877681,
        "grad_norm": 1.5292414426803589,
        "learning_rate": 1.0153349475383375e-05,
        "loss": 0.6068,
        "step": 630
    },
    {
        "epoch": 0.5083399523431295,
        "grad_norm": 1.3199759721755981,
        "learning_rate": 9.991928974979823e-06,
        "loss": 0.5756,
        "step": 640
    },
    {
        "epoch": 0.5162827640984908,
        "grad_norm": 1.614033579826355,
        "learning_rate": 9.830508474576272e-06,
        "loss": 0.6048,
        "step": 650
    },
    {
        "epoch": 0.5242255758538522,
        "grad_norm": 1.848564863204956,
        "learning_rate": 9.669087974172721e-06,
        "loss": 0.5818,
        "step": 660
    },
    {
        "epoch": 0.5321683876092137,
        "grad_norm": 1.696099877357483,
        "learning_rate": 9.50766747376917e-06,
        "loss": 0.5931,
        "step": 670
    },
    {
        "epoch": 0.5401111993645751,
        "grad_norm": 1.5973474979400635,
        "learning_rate": 9.346246973365619e-06,
        "loss": 0.6244,
        "step": 680
    },
    {
        "epoch": 0.5480540111199365,
        "grad_norm": 1.4849354028701782,
        "learning_rate": 9.184826472962068e-06,
        "loss": 0.6426,
        "step": 690
    },
    {
        "epoch": 0.5559968228752978,
        "grad_norm": 1.5516749620437622,
        "learning_rate": 9.023405972558515e-06,
        "loss": 0.598,
        "step": 700
    },
    {
        "epoch": 0.5639396346306592,
        "grad_norm": 1.7927989959716797,
        "learning_rate": 8.861985472154965e-06,
        "loss": 0.542,
        "step": 710
    },
    {
        "epoch": 0.5718824463860207,
        "grad_norm": 1.4942034482955933,
        "learning_rate": 8.700564971751413e-06,
        "loss": 0.5706,
        "step": 720
    },
    {
        "epoch": 0.5798252581413821,
        "grad_norm": 1.4357738494873047,
        "learning_rate": 8.539144471347861e-06,
        "loss": 0.6001,
        "step": 730
    },
    {
        "epoch": 0.5877680698967435,
        "grad_norm": 1.5651885271072388,
        "learning_rate": 8.37772397094431e-06,
        "loss": 0.5833,
        "step": 740
    },
    {
        "epoch": 0.5957108816521048,
        "grad_norm": 1.5565036535263062,
        "learning_rate": 8.216303470540759e-06,
        "loss": 0.5731,
        "step": 750
    },
    {
        "epoch": 0.6036536934074662,
        "grad_norm": 1.3809224367141724,
        "learning_rate": 8.054882970137208e-06,
        "loss": 0.5952,
        "step": 760
    },
    {
        "epoch": 0.6115965051628276,
        "grad_norm": 1.6755388975143433,
        "learning_rate": 7.893462469733657e-06,
        "loss": 0.5906,
        "step": 770
    },
    {
        "epoch": 0.6195393169181891,
        "grad_norm": 1.5834680795669556,
        "learning_rate": 7.732041969330106e-06,
        "loss": 0.5747,
        "step": 780
    },
    {
        "epoch": 0.6274821286735505,
        "grad_norm": 1.5550415515899658,
        "learning_rate": 7.5706214689265545e-06,
        "loss": 0.5662,
        "step": 790
    },
    {
        "epoch": 0.6354249404289118,
        "grad_norm": 1.4043807983398438,
        "learning_rate": 7.4092009685230025e-06,
        "loss": 0.5248,
        "step": 800
    },
    {
        "epoch": 0.6433677521842732,
        "grad_norm": 2.178830862045288,
        "learning_rate": 7.247780468119452e-06,
        "loss": 0.56,
        "step": 810
    },
    {
        "epoch": 0.6513105639396346,
        "grad_norm": 1.5506685972213745,
        "learning_rate": 7.0863599677159e-06,
        "loss": 0.6304,
        "step": 820
    },
    {
        "epoch": 0.659253375694996,
        "grad_norm": 1.7525118589401245,
        "learning_rate": 6.924939467312349e-06,
        "loss": 0.5562,
        "step": 830
    },
    {
        "epoch": 0.6671961874503575,
        "grad_norm": 1.6323055028915405,
        "learning_rate": 6.763518966908798e-06,
        "loss": 0.5758,
        "step": 840
    },
    {
        "epoch": 0.6751389992057188,
        "grad_norm": 1.6851602792739868,
        "learning_rate": 6.602098466505247e-06,
        "loss": 0.5642,
        "step": 850
    },
    {
        "epoch": 0.6830818109610802,
        "grad_norm": 1.8220422267913818,
        "learning_rate": 6.440677966101695e-06,
        "loss": 0.6032,
        "step": 860
    },
    {
        "epoch": 0.6910246227164416,
        "grad_norm": 1.491424560546875,
        "learning_rate": 6.279257465698144e-06,
        "loss": 0.583,
        "step": 870
    },
    {
        "epoch": 0.698967434471803,
        "grad_norm": 1.3730125427246094,
        "learning_rate": 6.117836965294592e-06,
        "loss": 0.5643,
        "step": 880
    },
    {
        "epoch": 0.7069102462271644,
        "grad_norm": 1.5747658014297485,
        "learning_rate": 5.956416464891041e-06,
        "loss": 0.6159,
        "step": 890
    },
    {
        "epoch": 0.7148530579825259,
        "grad_norm": 1.630825161933899,
        "learning_rate": 5.794995964487491e-06,
        "loss": 0.5443,
        "step": 900
    },
    {
        "epoch": 0.7227958697378872,
        "grad_norm": 1.9752225875854492,
        "learning_rate": 5.633575464083939e-06,
        "loss": 0.5919,
        "step": 910
    },
    {
        "epoch": 0.7307386814932486,
        "grad_norm": 1.6646333932876587,
        "learning_rate": 5.472154963680389e-06,
        "loss": 0.5758,
        "step": 920
    },
    {
        "epoch": 0.73868149324861,
        "grad_norm": 1.7336204051971436,
        "learning_rate": 5.310734463276837e-06,
        "loss": 0.5928,
        "step": 930
    },
    {
        "epoch": 0.7466243050039714,
        "grad_norm": 1.592905044555664,
        "learning_rate": 5.1493139628732855e-06,
        "loss": 0.5699,
        "step": 940
    },
    {
        "epoch": 0.7545671167593329,
        "grad_norm": 1.5767135620117188,
        "learning_rate": 4.987893462469734e-06,
        "loss": 0.5929,
        "step": 950
    },
    {
        "epoch": 0.7625099285146942,
        "grad_norm": 1.8332459926605225,
        "learning_rate": 4.826472962066182e-06,
        "loss": 0.5296,
        "step": 960
    },
    {
        "epoch": 0.7704527402700556,
        "grad_norm": 1.3837159872055054,
        "learning_rate": 4.665052461662631e-06,
        "loss": 0.5737,
        "step": 970
    },
    {
        "epoch": 0.778395552025417,
        "grad_norm": 1.6761122941970825,
        "learning_rate": 4.50363196125908e-06,
        "loss": 0.4982,
        "step": 980
    },
    {
        "epoch": 0.7863383637807784,
        "grad_norm": 1.5168102979660034,
        "learning_rate": 4.342211460855529e-06,
        "loss": 0.5057,
        "step": 990
    },
    {
        "epoch": 0.7942811755361397,
        "grad_norm": 1.593683123588562,
        "learning_rate": 4.180790960451978e-06,
        "loss": 0.5387,
        "step": 1000
    },
    {
        "epoch": 0.8022239872915012,
        "grad_norm": 1.8265231847763062,
        "learning_rate": 4.0193704600484266e-06,
        "loss": 0.5629,
        "step": 1010
    },
    {
        "epoch": 0.8101667990468626,
        "grad_norm": 2.0821692943573,
        "learning_rate": 3.8579499596448746e-06,
        "loss": 0.5424,
        "step": 1020
    },
    {
        "epoch": 0.818109610802224,
        "grad_norm": 1.8064472675323486,
        "learning_rate": 3.6965294592413243e-06,
        "loss": 0.5392,
        "step": 1030
    },
    {
        "epoch": 0.8260524225575854,
        "grad_norm": 1.86802077293396,
        "learning_rate": 3.535108958837773e-06,
        "loss": 0.592,
        "step": 1040
    },
    {
        "epoch": 0.8339952343129468,
        "grad_norm": 1.5141528844833374,
        "learning_rate": 3.3736884584342215e-06,
        "loss": 0.5609,
        "step": 1050
    },
    {
        "epoch": 0.8419380460683081,
        "grad_norm": 1.9337353706359863,
        "learning_rate": 3.2122679580306704e-06,
        "loss": 0.5238,
        "step": 1060
    },
    {
        "epoch": 0.8498808578236696,
        "grad_norm": 1.8822307586669922,
        "learning_rate": 3.0508474576271192e-06,
        "loss": 0.5804,
        "step": 1070
    },
    {
        "epoch": 0.857823669579031,
        "grad_norm": 1.6388540267944336,
        "learning_rate": 2.8894269572235677e-06,
        "loss": 0.538,
        "step": 1080
    },
    {
        "epoch": 0.8657664813343924,
        "grad_norm": 1.9420630931854248,
        "learning_rate": 2.7280064568200165e-06,
        "loss": 0.5744,
        "step": 1090
    },
    {
        "epoch": 0.8737092930897538,
        "grad_norm": 1.8501287698745728,
        "learning_rate": 2.5665859564164654e-06,
        "loss": 0.5292,
        "step": 1100
    },
    {
        "epoch": 0.8816521048451151,
        "grad_norm": 1.5249311923980713,
        "learning_rate": 2.4051654560129138e-06,
        "loss": 0.5452,
        "step": 1110
    },
    {
        "epoch": 0.8895949166004765,
        "grad_norm": 1.8399567604064941,
        "learning_rate": 2.2437449556093626e-06,
        "loss": 0.5557,
        "step": 1120
    },
    {
        "epoch": 0.897537728355838,
        "grad_norm": 1.8701040744781494,
        "learning_rate": 2.0823244552058115e-06,
        "loss": 0.5682,
        "step": 1130
    },
    {
        "epoch": 0.9054805401111994,
        "grad_norm": 2.103286027908325,
        "learning_rate": 1.92090395480226e-06,
        "loss": 0.5745,
        "step": 1140
    },
    {
        "epoch": 0.9134233518665608,
        "grad_norm": 1.5847302675247192,
        "learning_rate": 1.7594834543987087e-06,
        "loss": 0.5847,
        "step": 1150
    },
    {
        "epoch": 0.9213661636219221,
        "grad_norm": 1.598717451095581,
        "learning_rate": 1.5980629539951576e-06,
        "loss": 0.5314,
        "step": 1160
    },
    {
        "epoch": 0.9293089753772835,
        "grad_norm": 1.7404624223709106,
        "learning_rate": 1.4366424535916062e-06,
        "loss": 0.5758,
        "step": 1170
    },
    {
        "epoch": 0.937251787132645,
        "grad_norm": 1.6088777780532837,
        "learning_rate": 1.275221953188055e-06,
        "loss": 0.5529,
        "step": 1180
    },
    {
        "epoch": 0.9451945988880064,
        "grad_norm": 1.8292778730392456,
        "learning_rate": 1.1138014527845037e-06,
        "loss": 0.5799,
        "step": 1190
    },
    {
        "epoch": 0.9531374106433678,
        "grad_norm": 1.7994470596313477,
        "learning_rate": 9.523809523809525e-07,
        "loss": 0.6373,
        "step": 1200
    },
    {
        "epoch": 0.9610802223987291,
        "grad_norm": 1.656332015991211,
        "learning_rate": 7.909604519774013e-07,
        "loss": 0.4967,
        "step": 1210
    },
    {
        "epoch": 0.9690230341540905,
        "grad_norm": 2.034456253051758,
        "learning_rate": 6.295399515738499e-07,
        "loss": 0.5964,
        "step": 1220
    },
    {
        "epoch": 0.9769658459094519,
        "grad_norm": 1.7358042001724243,
        "learning_rate": 4.681194511702987e-07,
        "loss": 0.5504,
        "step": 1230
    },
    {
        "epoch": 0.9849086576648134,
        "grad_norm": 1.4935163259506226,
        "learning_rate": 3.066989507667474e-07,
        "loss": 0.5699,
        "step": 1240
    },
    {
        "epoch": 0.9928514694201748,
        "grad_norm": 1.9214606285095215,
        "learning_rate": 1.4527845036319614e-07,
        "loss": 0.6282,
        "step": 1250
    }
]