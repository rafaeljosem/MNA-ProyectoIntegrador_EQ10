{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPIZA6lu1CZYKp235/mD9H/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafaeljosem/MNA-ProyectoIntegrador_EQ10/blob/main/finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación de dependencias\n",
        "!pip install transformers datasets peft accelerate evaluate nltk numpy pandas"
      ],
      "metadata": {
        "id": "B6ui7vbLfaeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from accelerate import Accelerator\n",
        "import evaluate\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "%load_ext cudf.pandas"
      ],
      "metadata": {
        "id": "YUAdpWlOfmW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Cbzrj9t4fy7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga y preprocesamiento de datos\n",
        "df = pd.read_csv('MexicanLaws_Clean_Compiled_PrePro_DataSet.csv')\n",
        "df = df[['File Name', 'Text', 'Tokens']]\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "df.columns = ['file_name', 'text', 'tokens']"
      ],
      "metadata": {
        "id": "SFd5ycNKf1eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenización y formateo de datos\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "uz9IpSGff3WE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(text):\n",
        "  return f\"<s>[INST] {text}  [/INST]\"\n",
        "\n",
        "def format_completion(text):\n",
        "  return f\"[RESP] {text} [/RESP]\"\n",
        "\n",
        "def tokenize(examples):\n",
        "  prompts = [format_prompt(text) for text in examples['text']]\n",
        "  completions = [format_completion(text) for text in examples['text']]\n",
        "\n",
        "  tokenized_prompts = tokenizer(prompts, max_length=512, truncation=True)\n",
        "  tokenized_completions = tokenizer(completions, max_length=512, truncation=True)\n",
        "\n",
        "  examples['input_ids'] = tokenized_prompts['input_ids']\n",
        "  examples['labels'] = tokenized_completions['input_ids']\n",
        "\n",
        "  return examples"
      ],
      "metadata": {
        "id": "Bi66Sxo7f57c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset.from_pandas(df)\n",
        "dataset = dataset.map(tokenize, batched=True, remove_columns=['file_name', 'text', 'tokens'])\n",
        "\n",
        "train_dataset = dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = train_dataset['train']\n",
        "val_dataset = dataset['test']"
      ],
      "metadata": {
        "id": "lpOwJos5f8u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga del modelo base\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  model_name,\n",
        "  load_in_8bit=True,\n",
        "  device_map='auto',\n",
        ")"
      ],
      "metadata": {
        "id": "1ubjZpiEf_Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración de LoRA\n",
        "lora_config = LoraConfig(\n",
        "  r=16,\n",
        "  lora_alpha=32,\n",
        "  lora_dropout=0.05,\n",
        "  bias=\"none\",\n",
        "  task_type=\"CAUSAL_LM\"\n",
        ")"
      ],
      "metadata": {
        "id": "2RRFuwPUgAWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "22ZuEBLqgCLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparación del entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "  output_dir='./lora-mexican-laws',\n",
        "  learning_rate=3e-4,\n",
        "  num_train_epochs=3,\n",
        "  per_device_train_batch_size=4,\n",
        "  per_device_eval_batch_size=4,\n",
        "  weight_decay=0.02,\n",
        "  evaluation_strategy='steps',\n",
        "  eval_steps=200,\n",
        "  save_strategy='steps',\n",
        "  save_steps=200,\n",
        "  save_total_limit=3,\n",
        "  logging_steps=50,\n",
        "  report_to='wandb',\n",
        ")"
      ],
      "metadata": {
        "id": "vhPIQqJsgD6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "  model=model,\n",
        "  args=training_args,\n",
        "  train_dataset=train_dataset,\n",
        "  eval_dataset=val_dataset,\n",
        "  data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")"
      ],
      "metadata": {
        "id": "X9-OGj-WgFbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "H-wCPbfAgHvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluación\n",
        "metric = evaluate.load('rouge')"
      ],
      "metadata": {
        "id": "c1yYqhK8gJO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "  predictions, labels = eval_pred\n",
        "  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "  result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "  return result\n",
        "\n",
        "trainer.evaluate(eval_dataset=val_dataset, compute_metrics=compute_metrics)"
      ],
      "metadata": {
        "id": "WCu9D37-gPj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia\n",
        "def inference(prompt, max_length=512):\n",
        "  print(f\"Prompt: {prompt}\")\n",
        "  input_ids = tokenizer(format_prompt(prompt), return_tensors='pt').input_ids.cuda()\n",
        "  response = model.generate(input_ids, max_length=max_length)\n",
        "  print(f\"Response: {tokenizer.decode(response[0], skip_special_tokens=True)}\")\n",
        "\n",
        "inference(\"¿Quién escribió la Constitución Mexicana?\")"
      ],
      "metadata": {
        "id": "lPC6Xjq7gRRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFJDMKCvfXXM"
      },
      "outputs": [],
      "source": [
        "# Empaquetado y compartición\n",
        "model.save_pretrained('./lora-mexican-laws')\n",
        "tokenizer.save_pretrained('./lora-mexican-laws')"
      ]
    }
  ]
}